{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30083fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import cupyx.scipy.sparse as sp\n",
    "import torch.backends.cudnn as cudnn\n",
    "from cupyx.scipy.sparse import coo_matrix as cp_coo_matrix\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from cupyx.scipy.sparse import coo_matrix, dia_matrix, csr_matrix, triu, diags\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "cudnn.benchmark = False  \n",
    "cudnn.deterministic = True \n",
    "devices=torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_collate_fn(batch):\n",
    "    features, adj_labels, train_patient = zip(*batch)\n",
    "    features_tensor = torch.stack(features)\n",
    "    adj_labels_tensor = torch.stack(adj_labels)\n",
    "    train_patient_tensor = train_patient  \n",
    "    return features_tensor, adj_labels_tensor, train_patient_tensor\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not isinstance(sparse_mx, coo_matrix):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = cp.vstack((sparse_mx.row, sparse_mx.col)).T\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = coo_matrix(adj)\n",
    "    adj_ = adj + dia_matrix((cp.ones(adj.shape[0]), [0]), shape=adj.shape)\n",
    "    rowsum = cp.array(adj_.sum(axis=1)).flatten()\n",
    "    degree_mat_inv_sqrt = diags(cp.power(rowsum, -0.5))\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "def mask_test_edges(adj, ratio_val):\n",
    "    if isinstance(adj, cp.ndarray):\n",
    "        adj = csr_matrix(adj)\n",
    "    elif isinstance(adj, torch.Tensor): \n",
    "        adj = cp.asarray(adj.cpu().numpy())\n",
    "        adj = cp.sparse.csr_matrix(adj)\n",
    "    adj = adj - dia_matrix((adj.diagonal()[cp.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "    assert cp.diag(adj.toarray()).sum() == 0\n",
    "\n",
    "    adj_triu = triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "\n",
    "    num_val = int(cp.floor(edges.shape[0] * ratio_val))\n",
    "    all_edge_idx = cp.arange(edges.shape[0])\n",
    "    cp.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    mask = cp.ones(edges.shape[0], dtype=bool)\n",
    "    mask[val_edge_idx] = False\n",
    "    train_edges = edges[mask]\n",
    "    data = cp.ones(train_edges.shape[0])\n",
    "    adj_train = csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    return adj_train, train_edges, val_edges\n",
    "\n",
    "class MultiHeadAttentionModule(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.5):\n",
    "        super(MultiHeadAttentionModule, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout,\n",
    "                                                    batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if isinstance(query, torch.sparse.Tensor):\n",
    "            query = query.to_dense()\n",
    "\n",
    "        if isinstance(key, torch.sparse.Tensor):\n",
    "            key = key.to_dense()\n",
    "\n",
    "        if isinstance(value, torch.sparse.Tensor):\n",
    "            value = value.to_dense()\n",
    "        attn_output, attn_weights = self.multihead_attn(query, key, value, attn_mask=mask)\n",
    "        output = self.layer_norm(query + self.dropout(attn_output))\n",
    "        output = self.fc(output)\n",
    "        return output, attn_weights\n",
    "\n",
    "class GraphConvSparse(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, adj, activation=F.relu,device=devices, **kwargs):\n",
    "        super(GraphConvSparse, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weight = glorot_init(input_dim, output_dim).to(device)\n",
    "        self.adj = adj\n",
    "        self.activation = activation\n",
    "        self.device=device\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = x.squeeze(0)\n",
    "        x = x.to(self.device, dtype=torch.float32)\n",
    "        adj_dense = self.adj.clone().detach().to(self.device, dtype=torch.float32)\n",
    "        adj_dense = adj_dense.to_sparse().to(torch.float32)\n",
    "        self.weight=self.weight.to(torch.float32) \n",
    "        x = x.to(torch.float32) \n",
    "        x = torch.sparse.mm(adj_dense, torch.mm(x, self.weight))\n",
    "        outputs = self.activation(x)\n",
    "        return outputs\n",
    "\n",
    "def dot_product_decode(Z):\n",
    "    return torch.sigmoid(torch.mm(Z, Z.t()))\n",
    "\n",
    "def glorot_init(input_dim, output_dim):\n",
    "    init_range = torch.sqrt(torch.tensor(6.0 / (input_dim + output_dim), device=devices))\n",
    "    initial = cp.random.uniform(-init_range, init_range, size=(input_dim, output_dim))\n",
    "    initial_gpu = cp.asarray(initial).get()\n",
    "    return nn.Parameter(torch.tensor(initial_gpu, device=devices, dtype=torch.float32), requires_grad=True)\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class EarlyStopping:\n",
    "\n",
    "    def __init__(self, save_path, patience=10, verbose=False, delta=0):\n",
    "\n",
    "        self.save_path = save_path\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = cp.inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        path = os.path.join(self.save_path, 'best_network.pth')\n",
    "        torch.save(model.state_dict(), path) \n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=1):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.05),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.LeakyReLU(negative_slope=0.05),\n",
    "            nn.Linear(32, 8),\n",
    "            nn.LeakyReLU(negative_slope=0.05),\n",
    "            nn.Linear(8, latent_dim), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 8),\n",
    "            nn.LeakyReLU(negative_slope=0.05),\n",
    "            nn.Linear(8, 32),\n",
    "            nn.LeakyReLU(negative_slope=0.05),\n",
    "            nn.Linear(32, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.05),\n",
    "            nn.Linear(128, input_dim), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "class GAEWithAttention(nn.Module):\n",
    "    def __init__(self, adj, input_dim, hidden1_dim, hidden2_dim, num_heads,device=devices):\n",
    "\n",
    "        super(GAEWithAttention, self).__init__()\n",
    "        self.adj = adj.to_sparse().to(device)\n",
    "        self.attention = MultiHeadAttentionModule(embed_dim=input_dim, num_heads=num_heads).to(device)\n",
    "        self.gcn1 = GraphConvSparse(input_dim, hidden1_dim, self.adj).to(device)\n",
    "        self.gcn2 = GraphConvSparse(hidden1_dim, hidden2_dim, self.adj, activation=Identity()).to(device)\n",
    "        self.device=device\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = x.to(self.device)\n",
    "        z_output,z_atten = self.attention(x, x, x)\n",
    "        hidden = self.gcn1(z_output)\n",
    "        z = self.gcn2(hidden)\n",
    "        z = z.unsqueeze(0)\n",
    "        return z,z_atten\n",
    "\n",
    "    def decode(self, z):\n",
    "        return dot_product_decode(z.squeeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        z,z_atten = self.encode(x)\n",
    "        a_pred = self.decode(z)\n",
    "        return a_pred, z, z_atten\n",
    "\n",
    "def GAE_function_with_attention_EP(net_m, feature_m_list, GAE_epochs=30000, learning_rate=0.01, num_heads=4,\n",
    "                                   ratio_val=0, seed=666, hidden1_dim=10, hidden2_dim=1, save_path=None, patience=10,device=devices):\n",
    "    patients=feature_m_list.keys()\n",
    "    a=list(feature_m_list.keys())[0]\n",
    "    input_dim = int(feature_m_list[a].shape[1])\n",
    "    \n",
    "    def convert_to_sparse(matrix):\n",
    "        if isinstance(matrix, torch.Tensor):\n",
    "            matrix = matrix.cpu().numpy() \n",
    "            matrix = cp.asarray(matrix) \n",
    "        row, col = cp.nonzero(matrix)  \n",
    "        data = matrix[row, col] \n",
    "        data = data.astype(cp.float32)  \n",
    "        coo_matrix = cp.sparse.coo_matrix((data, (row, col)), shape=matrix.shape)\n",
    "        return coo_matrix\n",
    "    net_m_copy=net_m\n",
    "    net_m =convert_to_sparse(cp.asarray(net_m))\n",
    "    adj = cp.sparse.csr_matrix(net_m)\n",
    "    adj_train, train_edges, val_edges = mask_test_edges(net_m, ratio_val=ratio_val)\n",
    "    adj_norm = preprocess_graph(net_m)\n",
    "    eye_matrix = sp.eye(adj_train.shape[0], dtype=adj_train.dtype, format='csr')\n",
    "    adj_label = adj_train + eye_matrix\n",
    "    adj_label = sparse_to_tuple(adj_label)\n",
    "    adj_norm = torch.sparse_coo_tensor(torch.LongTensor(adj_norm[0].T),\n",
    "                                            torch.FloatTensor(adj_norm[1]),\n",
    "                                            torch.Size(adj_norm[2]))\n",
    "    adj_label = torch.sparse_coo_tensor(torch.LongTensor(adj_label[0].T),\n",
    "                                        torch.FloatTensor(adj_label[1]),\n",
    "                                        torch.Size(adj_label[2]))\n",
    "    adj_sum = adj_train.sum()\n",
    "    adj_sum = adj_sum if adj_sum > 0 else 1e-6\n",
    "    pos_weight = (adj_train.shape[0] ** 2 - adj_sum) / adj_sum\n",
    "    norm = adj_train.shape[0] ** 2 / ((adj_train.shape[0] ** 2 - adj_sum) * 2)\n",
    "\n",
    "    weight_mask = adj_label.to_dense().view(-1) == 1\n",
    "    pos_weight = torch.tensor(pos_weight, device=device).float()\n",
    "    weight_mask = weight_mask.clone().detach().to(device)\n",
    "    weight_tensor = torch.ones(weight_mask.size(0), device=device)\n",
    "    weight_tensor[weight_mask] = pos_weight\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    model = GAEWithAttention(adj=adj_norm, input_dim=input_dim, hidden1_dim=hidden1_dim, hidden2_dim=hidden2_dim,\n",
    "                             num_heads=num_heads).to(device)\n",
    "    optimizer_A = Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    best_epoch_loss = float(\"inf\")\n",
    "    train_losses = []\n",
    "    early_stopping = EarlyStopping(patience=patience,delta=1e-6, save_path=save_path)\n",
    "    adj_norm = adj_norm.to(device)\n",
    "    adj_label = adj_label.to(device)\n",
    "    norm = torch.tensor(norm, device=device, dtype=torch.float32)\n",
    "    weight_tensor = weight_tensor.to(device)\n",
    "    best_z = None\n",
    "    best_epoch = 0\n",
    "    best_model = None\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    feature_all={}\n",
    "    for patient in patients:\n",
    "        feature_m=feature_m_list[patient]\n",
    "        df_b_reindexed = pd.DataFrame(cp.asnumpy(feature_m)).reindex(index=pd.DataFrame(cp.asnumpy(net_m_copy)).index).fillna(0).values\n",
    "        feature_m = df_b_reindexed\n",
    "        feature_m = convert_to_sparse(cp.asarray(feature_m))\n",
    "        feature_m = sp.csr_matrix(feature_m)\n",
    "        features = sparse_to_tuple(feature_m.tocoo())\n",
    "        \n",
    "        features = torch.sparse_coo_tensor(torch.LongTensor(features[0].T),\n",
    "                                            torch.FloatTensor(features[1]),\n",
    "                                            torch.Size(features[2]))  \n",
    "        features = features.to(device)\n",
    "        feature_all.update({patient:features})\n",
    "\n",
    "    for epoch in range(GAE_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for patient in patients:\n",
    "            features=feature_all[patient]\n",
    "            A_pred, z, _ = model(x=features)\n",
    "            loss_1=reconstruction_loss(A_pred.view(-1), adj_label.to_dense().view(-1))\n",
    "            optimizer_A.zero_grad()\n",
    "            loss_1.backward()\n",
    "            optimizer_A.step()\n",
    "            epoch_loss += loss_1.item()\n",
    "        epoch_loss = epoch_loss/len(patients)\n",
    "        if epoch_loss < best_epoch_loss:\n",
    "            best_epoch_loss = epoch_loss\n",
    "            best_model = model\n",
    "            best_epoch = epoch\n",
    "            min_loss_val = epoch_loss\n",
    "            patience_count = 0\n",
    "        else:\n",
    "            patience_count += 1\n",
    "        \n",
    "        if patience_count > patience:\n",
    "            break\n",
    "\n",
    "    return best_model, min_loss_val, best_epoch,feature_all\n",
    "\n",
    "def train_pathway_model(pathways_matrix,features_matrix_list,GAE_epochs=30000, learning_rate=0.01, num_heads=4,\n",
    "                                   ratio_val=0, seed=666, hidden1_dim=10, hidden2_dim=1, save_path=None, patience=10,device=devices):\n",
    "    pathways=pathways_matrix.keys()\n",
    "    pathway_model={}\n",
    "    patient_feature={}\n",
    "    i=0\n",
    "    for path in pathways:\n",
    "        model,_,_,features=GAE_function_with_attention_EP(net_m=pathways_matrix[path], feature_m_list=features_matrix_list, GAE_epochs=GAE_epochs, learning_rate=learning_rate, num_heads=num_heads,\n",
    "                                   ratio_val=ratio_val, seed=seed, hidden1_dim=hidden1_dim, hidden2_dim=hidden2_dim, save_path=save_path, patience=patience,device=devices)\n",
    "        pathway_model.update({path:model})\n",
    "        patient_feature.update({path:features})\n",
    "        i=i+1\n",
    "        print(f\"Pathway {i}/{len(pathways)}\")\n",
    "    return pathway_model,patient_feature\n",
    "\n",
    "def calculate_pathway_activity_3d(pathway_model, patient_feature, latent_dim=4, device='cuda:0'):\n",
    "\n",
    "    pathways = list(pathway_model.keys())\n",
    "    a = list(patient_feature.keys())[0]\n",
    "    patients = list(patient_feature[a].keys())\n",
    "    \n",
    "    all_pathways_activity= []\n",
    "    columns_path=[]\n",
    "    i = 0\n",
    "    for _, path in enumerate(pathways):\n",
    "        model = pathway_model[path].cuda()\n",
    "        features = patient_feature[path]\n",
    "        sample_pathway_dim=[]\n",
    "        for _, patient in enumerate(patients):\n",
    "            pat_feature = features[patient]\n",
    "            _, z, _= model(x=pat_feature)\n",
    "            z = z.squeeze(0)\n",
    "            model1 = Autoencoder(z.shape[0], latent_dim)\n",
    "            model1 = model1.to(device)\n",
    "            criterion = nn.MSELoss().cuda()\n",
    "            optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
    "            \n",
    "            aaa = torch.tensor(z.T, dtype=torch.float32).cuda()\n",
    "            num_epochs = 50\n",
    "            best_encoded = None\n",
    "            best_train_loss = float('inf')\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                model1.train()\n",
    "                optimizer.zero_grad()\n",
    "                encoded, decoded = model1(aaa)\n",
    "                loss = criterion(decoded, aaa)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if loss.item() < best_train_loss:\n",
    "                    best_train_loss = loss.item()\n",
    "                    best_encoded = encoded.detach().cpu().numpy()  \n",
    "                    \n",
    "            sample_pathway_dim.append(best_encoded.squeeze())\n",
    "        sample_pathway_dim = np.stack(sample_pathway_dim, axis=0)\n",
    "        all_pathways_activity.append(sample_pathway_dim)\n",
    "        column_names = [f\"{path}_latent_{i+1}\" for i in range(latent_dim)]\n",
    "        columns_path.extend(column_names)\n",
    "        i += 1\n",
    "        print(f\"Processed Pathway {i}/{len(pathways)}\")\n",
    "        \n",
    "    flattened_pathways_activity = torch.tensor(np.stack(all_pathways_activity, axis=1), dtype=torch.float32)\n",
    "    return flattened_pathways_activity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/pathways_adjacency_matrix_without_disease.pkl\", \"rb\") as f:\n",
    "    pathways_matrix = pickle.load(f)\n",
    "\n",
    "with open(\"Data/Liu/com_data_144_3.pkl\", \"rb\") as f:\n",
    "    com_data = pickle.load(f)\n",
    "    \n",
    "pathway_model_144, patient_feature_144 = train_pathway_model(pathways_matrix = pathways_matrix, features_matrix_list = com_data, GAE_epochs = 30000, learning_rate = 0.01, num_heads = 1,\n",
    "                                   ratio_val = 0, seed = 666, hidden1_dim = 3, hidden2_dim = 1, save_path = 'Data', patience = 20,device = devices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5314df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/Liu//patient_feature_144.pkl\", \"rb\") as f:\n",
    "    patient_feature_144 = pickle.load(f)\n",
    "\n",
    "with open(\"Data/Liu//pathway_model_144.pkl\", \"rb\") as f:\n",
    "    pathway_model_144 = pickle.load(f)\n",
    "\n",
    "pathway_144_activity = calculate_pathway_activity_3d(pathway_model = pathway_model_144, patient_feature = patient_feature_144, latent_dim = 6)\n",
    "\n",
    "with open(f'Data/Liu//pathway_144_activity.pkl', \"wb\") as f:\n",
    "    pickle.dump(pathway_144_activity, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
